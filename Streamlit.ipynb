{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apekshaa2908/Enhancing-E-Commerce-Agility-With-Advanced-ETL-Pipeline/blob/main/Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install boto3"
      ],
      "metadata": {
        "id": "fCyv7_67ExYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import boto3\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "import time\n",
        "\n",
        "# Function to read AWS credentials from a text file\n",
        "def read_credentials(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        credentials = {}\n",
        "        for line in lines:\n",
        "            key, value = line.split(':', 1)\n",
        "            credentials[key.strip()] = value.strip()\n",
        "    return credentials\n",
        "\n",
        "# Load AWS credentials from a file\n",
        "credentials = read_credentials('path/to/your/credentials.txt')\n",
        "\n",
        "# Assigning the credentials\n",
        "aws_access_key_id = credentials['Access Key']\n",
        "aws_secret_access_key = credentials['Secret Access Key']\n",
        "region_name = credentials['Region']\n",
        "\n",
        "# Define bucket names and Glue job name\n",
        "ORDER_BUCKET = 'order-apekshaa'\n",
        "RETURN_BUCKET = 'return-apekshaa'\n",
        "GLUE_JOB_NAME = 'order_return_glue_etl'  # The name of the Glue job to monitor\n",
        "\n",
        "# Initialize the S3 and Glue clients\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key,\n",
        "    region_name=region_name\n",
        ")\n",
        "\n",
        "glue = boto3.client(\n",
        "    'glue',\n",
        "    aws_access_key_id=aws_access_key_id,\n",
        "    aws_secret_access_key=aws_secret_access_key,\n",
        "    region_name=region_name\n",
        ")\n",
        "\n",
        "# Function to upload files to S3\n",
        "def upload_to_s3(file, bucket_name, file_name):\n",
        "    try:\n",
        "        file_buffer = BytesIO()\n",
        "        file.to_csv(file_buffer, index=False)\n",
        "        file_buffer.seek(0)\n",
        "        s3.put_object(Bucket=bucket_name, Key=file_name, Body=file_buffer.getvalue())\n",
        "        st.success(f'File {file_name} uploaded to {bucket_name}')\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error uploading {file_name}: {str(e)}\")\n",
        "\n",
        "# Function to check Glue job status\n",
        "def check_glue_job_status(job_run_id):\n",
        "    response = glue.get_job_run(JobName=GLUE_JOB_NAME, RunId=job_run_id)\n",
        "    return response['JobRun']['JobRunState']\n",
        "\n",
        "# File uploaders\n",
        "order_file = st.file_uploader(\"Upload order_table CSV\", type=[\"csv\"], key=\"order_file\")\n",
        "return_file = st.file_uploader(\"Upload return_table CSV\", type=[\"csv\"], key=\"return_file\")\n",
        "\n",
        "# Upload button for both files\n",
        "if order_file and return_file:\n",
        "    if st.button(\"Upload Both Files\"):\n",
        "        # Upload both files to S3\n",
        "        order_df = pd.read_csv(order_file)\n",
        "        return_df = pd.read_csv(return_file)\n",
        "\n",
        "        upload_to_s3(order_df, ORDER_BUCKET, 'order_table.csv')\n",
        "        upload_to_s3(return_df, RETURN_BUCKET, 'return_table.csv')\n",
        "\n",
        "        st.success(\"Both files uploaded. Monitoring Glue job...\")\n",
        "\n",
        "        # Assume the Glue job runs automatically; we need to fetch the most recent job run ID\n",
        "        with st.spinner(\"Fetching the latest Glue job run ID...\"):\n",
        "            response = glue.get_job_runs(JobName=GLUE_JOB_NAME)\n",
        "            latest_run = response['JobRuns'][0]  # Get the most recent job run\n",
        "            job_run_id = latest_run['Id']\n",
        "            st.write(f'Monitoring Glue job with Run ID: {job_run_id}')\n",
        "\n",
        "            # Monitor the Glue job status\n",
        "            while True:\n",
        "                time.sleep(60)  # Check every minute\n",
        "                status = check_glue_job_status(job_run_id)\n",
        "                st.write(f'Current Glue job status: {status}')\n",
        "\n",
        "                if status in ['SUCCEEDED', 'FAILED', 'STOPPED']:\n",
        "                    st.success(f'Glue job has {\"succeeded\" if status == \"SUCCEEDED\" else \"failed\" if status == \"FAILED\" else \"stopped\"}!')\n",
        "                    break\n",
        "else:\n",
        "    st.warning(\"Please upload both order_table and return_table CSV files to proceed.\")\n"
      ],
      "metadata": {
        "id": "_IoP0YYw8KHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -q -O - ipv4.icanhazip.com\n",
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "ezy3HmrlErBi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
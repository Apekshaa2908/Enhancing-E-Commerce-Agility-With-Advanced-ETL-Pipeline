{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOujQYlgA3WGU7u+MUW0+/C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apekshaa2908/Enhancing-E-Commerce-Agility-With-Advanced-ETL-Pipeline/blob/main/GlueJob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRyBGatIdnqK"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "import sys\n",
        "import boto3\n",
        "import json\n",
        "import time\n",
        "from awsglue.context import GlueContext\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Creating a Spark session\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "# Define Redshift cluster and database information\n",
        "jdbc_url = \"jdbc:redshift://enter-your-cluster-url-here:5439/enter-your-db-name-here\"\n",
        "redshift_db = \"enter-your-db-name-here\"\n",
        "redshift_table = \"enter-your-target-redshift-table-here\"\n",
        "\n",
        "# Fetch credentials from AWS Secrets Manager\n",
        "def get_redshift_credentials(secret_arn):\n",
        "    client = boto3.client('secretsmanager')\n",
        "\n",
        "    try:\n",
        "        response = client.get_secret_value(SecretId=secret_arn)\n",
        "        secret = json.loads(response['SecretString'])\n",
        "        return secret['username'], secret['password']\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error retrieving secret: {e}\")\n",
        "\n",
        "# Trigger the Glue Crawler\n",
        "def trigger_crawler(crawler_name):\n",
        "    glue_client = boto3.client('glue')\n",
        "\n",
        "    # Start the crawler\n",
        "    try:\n",
        "        glue_client.start_crawler(Name=crawler_name)\n",
        "        print(f\"Crawler {crawler_name} started successfully.\")\n",
        "\n",
        "        # Wait for the crawler to complete\n",
        "        while True:\n",
        "            crawler_status = glue_client.get_crawler(Name=crawler_name)['Crawler']['State']\n",
        "            if crawler_status == 'READY':\n",
        "                print(f\"Crawler {crawler_name} completed.\")\n",
        "                break\n",
        "            else:\n",
        "                print(f\"Crawler {crawler_name} is running...\")\n",
        "                time.sleep(30)  # Check every 30 seconds\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error starting crawler: {e}\")\n",
        "\n",
        "# Define the secret ARN where Redshift credentials are stored\n",
        "secret_arn = \"enter-your-secret-arn-here\"  # Enter your AWS Secrets Manager secret ARN\n",
        "\n",
        "# Trigger the crawler\n",
        "crawler_name = \"enter-your-crawler-name-here\"  # Enter your Glue crawler name\n",
        "trigger_crawler(crawler_name)\n",
        "\n",
        "# Fetch the Redshift credentials (username and password)\n",
        "redshift_user, redshift_password = get_redshift_credentials(secret_arn)\n",
        "\n",
        "# Create the database properties for the Redshift connection\n",
        "db_properties = {\n",
        "    \"user\": redshift_user,\n",
        "    \"password\": redshift_password,\n",
        "    \"driver\": \"com.amazon.redshift.jdbc42.Driver\"\n",
        "}\n",
        "\n",
        "# Retrieve the orders data from Glue catalog\n",
        "orders_df = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"enter-your-database-name-here\",\n",
        "    table_name=\"enter-your-order-table-name-here\"\n",
        ").toDF()\n",
        "\n",
        "# Retrieve the returns data from Glue catalog\n",
        "returns_df = glueContext.create_dynamic_frame.from_catalog(\n",
        "    database=\"enter-your-database-name-here\",\n",
        "    table_name=\"enter-your-return-table-name-here\"\n",
        ").toDF()\n",
        "\n",
        "# Add an index to the DataFrames and filter out the first row (header)\n",
        "orders_df = orders_df.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0]).toDF(orders_df.columns)\n",
        "returns_df = returns_df.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0]).toDF(returns_df.columns)\n",
        "\n",
        "# Join the orders and returns dataframes on the 'order_id' column from the second row\n",
        "joined_df = orders_df.join(\n",
        "    returns_df,\n",
        "    orders_df[\"order_id\"] == returns_df[\"order_id\"],\n",
        "    'inner'\n",
        ").drop(returns_df[\"order_id\"])\n",
        "\n",
        "# Write the joined dataframe to Redshift\n",
        "joined_df.write.jdbc(\n",
        "    url=jdbc_url,\n",
        "    table=redshift_table,\n",
        "    mode=\"overwrite\",\n",
        "    properties=db_properties\n",
        ")\n",
        "\n",
        "print(\"Data successfully written to Redshift\")\n",
        "\n"
      ]
    }
  ]
}